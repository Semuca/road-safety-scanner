{"full-text-retrieval-response": {
    "scopus-eid": "2-s2.0-85200440099",
    "originalText": "serial JL 271506 291210 291817 291820 291862 291866 291870 291883 31 Expert Systems with Applications EXPERTSYSTEMSAPPLICATIONS 2024-08-06 2024-08-06 2024-08-06 2024-08-06 2024-08-30T10:59:38 1-s2.0-S0957417424016981 S0957-4174(24)01698-1 S0957417424016981 10.1016/j.eswa.2024.124831 S300 S300.1 FULL-TEXT 1-s2.0-S0957417424X00184 2024-08-30T10:29:01.147058Z 0 0 20241205 2024 2024-08-06T07:11:57.269929Z absattachment articleinfo articlenumber articletitlenorm authfirstinitialnorm authfirstsurnamenorm cid cids contenttype copyright crossmark dateloaded dateloadedtxt datesearch datesort dateupdated dco docsubtype doctype doi eid ewtransactionid hubeid indexeddate issn issnnorm itemstage itemtransactionid itemweight openaccess openarchive pg pgfirst pii piinorm pubdatestart pubdatetxt pubyr sortorder srctitle srctitlenorm srctype ssids alllist content subj subheadings suppl tomb volfirst volissue volumelist webpdf webpdfpagecount yearnav figure table body mmlmath affil articletitle auth authfirstini authfull authkeywords authlast highlightsabst primabst ref specialabst 0957-4174 09574174 true 256 256 C Volume 256 55 124831 124831 124831 20241205 5 December 2024 2024-12-05 2024 article fla © 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies. INTEGRATEDFRAMEWORKFORRAINFALLPREDICTIONANALYSISUSINGASTACKEDHETEROGENEOUSENSEMBLEMODELSHEM UMAMAHESWARI P 1 Introduction 2 Related work 3 Proposed model 3.1 Study area and dataset 3.2 Imputing missing values by Variable Specific Hot Deck (VSHD) 3.3 Feature selection 3.4 The Stacked Heterogeneous Ensemble Model (SHEM) 3.4.1 Computational complexity of proposed with traditional models 3.4.2 Specifications of hyperparameters for stacking ensemble models 3.5 Cross-Validation 4 Results and discussion 4.1 Experimental setup 4.2 Comparison of imputation methods 4.3 Prediction results by SHEM 4.4 XGBoost as a Meta-Classifier 5 Conclusion and future enhancement Exploration of additional base learners Dynamic ensemble adaptation Funding information CRediT authorship contribution statement References ADNAN 2021 597 616 R BARRERAANIMAS 2022 100204 A DASILVA 2022 108504 R DANANDEHMEHR 2023 10441 10455 A ESPEHOLT 2022 1 10 L EVANS 2020 104697 F FAHAD 2023 158760 S FLORES 2023 M CHAGAHI 2024 108345 M JOSE 2022 4678 D KO 2022 105072 J LI 2021 113191 J LUO 2024 120497 Y MOHAPATRA 2023 100133 S NARANG 2024 1495 U OUMA 2021 1 24 Y PANDA 2024 1 20 J PENALVO 2022 1 15 F RAHMAN 2022 3504 A RAMPAL 2022 100525 N REN 2021 100178 X RIBEIRO 2022 107712 M RIDWAN 2021 1651 1663 W WEI 2022 4003 4018 M WU 2020 137077 Z YIN 2023 162 174 H ZHANG 2020 2547 W UMAMAHESWARIX2024X124831 UMAMAHESWARIX2024X124831XP 2026-08-06T00:00:00.000Z 2026-08-06T00:00:00.000Z http://creativecommons.org/licenses/by-nc-nd/4.0/ © 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies. https://doi.org/10.15223/policy-017 https://doi.org/10.15223/policy-037 https://doi.org/10.15223/policy-012 https://doi.org/10.15223/policy-029 https://doi.org/10.15223/policy-004 item S0957-4174(24)01698-1 S0957417424016981 1-s2.0-S0957417424016981 10.1016/j.eswa.2024.124831 271506 2024-08-30T10:29:01.147058Z 2024-12-05 1-s2.0-S0957417424016981-main.pdf https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/MAIN/application/pdf/26d5ff564d797441783554b255e0d3c3/main.pdf main.pdf pdf true 3604034 MAIN 15 1-s2.0-S0957417424016981-main_1.png https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/PREVIEW/image/png/ad423caff5b49d9f1be37e8fe58d2734/main_1.png main_1.png png 59189 849 656 IMAGE-WEB-PDF 1 1-s2.0-S0957417424016981-gr2.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr2/DOWNSAMPLED/image/jpeg/d240ec3f5d8df91e3c2052a720c6b070/gr2.jpg gr2 gr2.jpg jpg 53840 345 552 IMAGE-DOWNSAMPLED 1-s2.0-S0957417424016981-ga1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/ga1/DOWNSAMPLED/image/jpeg/32a3b7a1bc6a7c7a349eefbd15ad43f2/ga1.jpg ga1 true ga1.jpg jpg 28016 200 415 IMAGE-DOWNSAMPLED 1-s2.0-S0957417424016981-gr3.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr3/DOWNSAMPLED/image/jpeg/264dd6415ad45af6ecc0835f35c16d66/gr3.jpg gr3 gr3.jpg jpg 22375 219 621 IMAGE-DOWNSAMPLED 1-s2.0-S0957417424016981-gr4.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr4/DOWNSAMPLED/image/jpeg/a507353266d0ad738da921d00c055ee2/gr4.jpg gr4 gr4.jpg jpg 58000 477 528 IMAGE-DOWNSAMPLED 1-s2.0-S0957417424016981-gr5.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr5/DOWNSAMPLED/image/jpeg/ee91107727f3696a35fa69af0251e288/gr5.jpg gr5 gr5.jpg jpg 18551 264 546 IMAGE-DOWNSAMPLED 1-s2.0-S0957417424016981-gr1.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr1/DOWNSAMPLED/image/jpeg/35e5059055372151cbabf3ef15074887/gr1.jpg gr1 gr1.jpg jpg 29284 279 638 IMAGE-DOWNSAMPLED 1-s2.0-S0957417424016981-gr6.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr6/DOWNSAMPLED/image/jpeg/53859ba779c746f2b9e1455251946191/gr6.jpg gr6 gr6.jpg jpg 71978 453 638 IMAGE-DOWNSAMPLED 1-s2.0-S0957417424016981-gr7.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr7/DOWNSAMPLED/image/jpeg/6992c174e0c491cd24ffa242bcf89e24/gr7.jpg gr7 gr7.jpg jpg 56664 535 651 IMAGE-DOWNSAMPLED 1-s2.0-S0957417424016981-gr2.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr2/THUMBNAIL/image/gif/3acf6d42477cca8c0d21d690938ca874/gr2.sml gr2 gr2.sml sml 12331 137 219 IMAGE-THUMBNAIL 1-s2.0-S0957417424016981-ga1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/ga1/THUMBNAIL/image/gif/7f1d9c60aea08d3c31287d892f96302f/ga1.sml ga1 true ga1.sml sml 12708 106 219 IMAGE-THUMBNAIL 1-s2.0-S0957417424016981-gr3.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr3/THUMBNAIL/image/gif/09fc3bbbbbe1ef6de44f5c5bde3c9cdb/gr3.sml gr3 gr3.sml sml 3958 77 219 IMAGE-THUMBNAIL 1-s2.0-S0957417424016981-gr4.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr4/THUMBNAIL/image/gif/cec81555144bb56d088920fae43ba03a/gr4.sml gr4 gr4.sml sml 9859 163 181 IMAGE-THUMBNAIL 1-s2.0-S0957417424016981-gr5.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr5/THUMBNAIL/image/gif/a0a93d8e79dff2db345d3437541dfdbd/gr5.sml gr5 gr5.sml sml 4169 106 219 IMAGE-THUMBNAIL 1-s2.0-S0957417424016981-gr1.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr1/THUMBNAIL/image/gif/4f370ca1699370a166edfc334a79d128/gr1.sml gr1 gr1.sml sml 5346 96 219 IMAGE-THUMBNAIL 1-s2.0-S0957417424016981-gr6.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/gr6/THUMBNAIL/image/gif/74ee2b5b28c54175cd6cb122c721c01a/gr6.sml gr6 gr6.sml sml 11350 156 219 IMAGE-THUMBNAIL 1-s2.0-S0957417424016981-gr7.sml https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424017317/gr7/THUMBNAIL/image/gif/d39e7c561f70e9746d49b6013a8e0da3/gr7.sml gr7 gr7.sml sml 7361 163 199 IMAGE-THUMBNAIL 1-s2.0-S0957417424016981-gr2_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/HIGHRES/image/jpeg/875b59741249cdd7787ded68fbdc2b1c/gr2_lrg.jpg gr2 gr2_lrg.jpg jpg 277965 1528 2443 IMAGE-HIGH-RES 1-s2.0-S0957417424016981-ga1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/HIGHRES/image/jpeg/c2cedaf5dd45cfe6b9f052b90f21fd09/ga1_lrg.jpg ga1 true ga1_lrg.jpg jpg 180928 886 1837 IMAGE-HIGH-RES 1-s2.0-S0957417424016981-gr3_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/HIGHRES/image/jpeg/1e84540ba4feb5d124d6a544eac59431/gr3_lrg.jpg gr3 gr3_lrg.jpg jpg 130859 970 2751 IMAGE-HIGH-RES 1-s2.0-S0957417424016981-gr4_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/HIGHRES/image/jpeg/c6371b49f02304f90f9f5d7945087c01/gr4_lrg.jpg gr4 gr4_lrg.jpg jpg 334614 2110 2337 IMAGE-HIGH-RES 1-s2.0-S0957417424016981-gr5_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/HIGHRES/image/jpeg/cc7033c986418ddb983e71f70373d2be/gr5_lrg.jpg gr5 gr5_lrg.jpg jpg 91685 1168 2418 IMAGE-HIGH-RES 1-s2.0-S0957417424016981-gr1_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/HIGHRES/image/jpeg/4525c589a84b2134177ac28f1646bb2f/gr1_lrg.jpg gr1 gr1_lrg.jpg jpg 158888 1236 2826 IMAGE-HIGH-RES 1-s2.0-S0957417424016981-gr6_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424017317/HIGHRES/image/jpeg/d1f306caf4cb09288d659158a90a8b6a/gr6_lrg.jpg gr6 gr6_lrg.jpg jpg 416255 2008 2827 IMAGE-HIGH-RES 1-s2.0-S0957417424016981-gr7_lrg.jpg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424017317/HIGHRES/image/jpeg/27320aec2f83c0b5ec55ba1e5b0d8412/gr7_lrg.jpg gr7 gr7_lrg.jpg jpg 314957 2367 2881 IMAGE-HIGH-RES 1-s2.0-S0957417424016981-si1.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/775f3e119808041df10260627005882b/si1.svg si1 si1.svg svg 12972 ALTIMG 1-s2.0-S0957417424016981-si10.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/cdba1f819e434b365b4714fa83984133/si10.svg si10 si10.svg svg 19799 ALTIMG 1-s2.0-S0957417424016981-si11.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/c78db7bf0dc2464880fecc483b27aaad/si11.svg si11 si11.svg svg 3553 ALTIMG 1-s2.0-S0957417424016981-si12.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/42b4922647dd8395fc8dd553952d3791/si12.svg si12 si12.svg svg 33267 ALTIMG 1-s2.0-S0957417424016981-si13.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/f41516a11ec50f4c9e440f8c6da658b3/si13.svg si13 si13.svg svg 11547 ALTIMG 1-s2.0-S0957417424016981-si14.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/49c1d02ffb211a5a94b35740e525d5f2/si14.svg si14 si14.svg svg 5361 ALTIMG 1-s2.0-S0957417424016981-si15.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/0ea7246d866bbbe8ccaee2724ee2a585/si15.svg si15 si15.svg svg 6992 ALTIMG 1-s2.0-S0957417424016981-si16.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/1a4cf07e84e3b3759fc1bba922221286/si16.svg si16 si16.svg svg 10085 ALTIMG 1-s2.0-S0957417424016981-si17.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/da8a707ebcf32d46e2ef5356abc6a8f6/si17.svg si17 si17.svg svg 19758 ALTIMG 1-s2.0-S0957417424016981-si18.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/c465fb9a801acf0d71bd41f3af3d2303/si18.svg si18 si18.svg svg 3620 ALTIMG 1-s2.0-S0957417424016981-si19.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/11cff05a906d3cf8cd101dd5154914cf/si19.svg si19 si19.svg svg 6798 ALTIMG 1-s2.0-S0957417424016981-si2.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/886ceb9f36c890ff77a52c916cdef272/si2.svg si2 si2.svg svg 14605 ALTIMG 1-s2.0-S0957417424016981-si20.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/4eaeaea7a04b94205210554710514726/si20.svg si20 si20.svg svg 3201 ALTIMG 1-s2.0-S0957417424016981-si3.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/5b50524163f353d0e11cb46cbb7a03df/si3.svg si3 si3.svg svg 9757 ALTIMG 1-s2.0-S0957417424016981-si4.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/f14059850d39985483de3ef0a807cc48/si4.svg si4 si4.svg svg 6450 ALTIMG 1-s2.0-S0957417424016981-si5.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/85fdbd93a4715c2ca472392675c675ca/si5.svg si5 si5.svg svg 8831 ALTIMG 1-s2.0-S0957417424016981-si6.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/05a3bc1eb83b210ffeb3a860c2e84ed1/si6.svg si6 si6.svg svg 8414 ALTIMG 1-s2.0-S0957417424016981-si7.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/b84967ccf4e1788d0fd09165d051fa27/si7.svg si7 si7.svg svg 9313 ALTIMG 1-s2.0-S0957417424016981-si8.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/75cac14b98aed5316e094cc458a86ecd/si8.svg si8 si8.svg svg 7605 ALTIMG 1-s2.0-S0957417424016981-si9.svg https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0957417424016981/image/svg+xml/47811fd38564d4ee8a03ac1587432900/si9.svg si9 si9.svg svg 20296 ALTIMG ESWA 124831 124831 S0957-4174(24)01698-1 10.1016/j.eswa.2024.124831 Elsevier Ltd Fig. 1 Architecture of proposed Stacked Heterogeneous Ensemble Model. Fig. 2 Location map of Cuddalore district [Ref: https://tnenvis.nic.in/files/CUDDALORE.pdf]. Fig. 3 Base and Meta classifiers in Stacked Heterogeneous Ensemble Model (SHEM). Fig. 4 (a and b) Comparison of different classifier before and after applying VSHD. Fig. 5 Comparison of imputation methods. Fig. 6 (a\u2013d) Prediction results by SHEM before and after preprocessing. Fig. 7 (a\u2013d) Learning curves of the SHEM with LSTM during training and validation. Table 1 Sample Dataset \u2013 Nov\u201923. YEAR Day wind speed Earth skin Temp Temp_Max Temp_Min Relative Humidity Precipitation Specific humidity Surface pressure Wind speed_Max Wind speed_Min Wind direction Rainfall 2023 1 4.85 25.13 26.16 23.8 78.06 0.48 15.1 101.3 6.79 4.95 47.94 yes 2023 2 4.33 24.76 25.83 23.4 77.69 0.46 14.7 101.26 6.07 4.63 43 yes 2023 3 3.84 24.38 25.51 22.3 78.56 0.24 14.3 101.24 5.42 4.06 45.5 yes 2023 4 3.54 24.17 25.05 22.1 76.75 0 13.8 101.25 4.7 3.92 51.94 no 2023 5 4.06 24.37 25.31 22.7 76.31 0.22 13.9 101.3 5.6 4.34 44.38 yes 2023 6 3.95 24.45 25.43 23.1 76.38 0.06 14.2 101.26 5.28 4.35 43.69 yes 2023 7 3.72 24.96 26.1 23.2 77.81 0.07 14.8 101.34 5.2 3.82 52.19 yes 2023 8 3.24 25.03 26.51 23.4 76.62 0.04 14.8 101.38 4.83 2.95 57.31 yes 2023 9 3.17 24.76 26.25 23 77.44 0.03 14.7 101.31 4.48 3.23 52.62 yes 2023 10 2.88 24.79 26 22.9 75.25 0.01 14.1 101.2 4.7 2.53 60.06 yes 2023 11 3 24.84 25.81 22.8 77.5 0.02 14.4 101.19 4.6 3.01 80.88 yes 2023 12 3.14 24.76 25.68 22.9 76.94 0.09 14.3 101.22 4.53 3.06 69.5 yes 2023 13 3.73 24.94 25.79 23 75.38 0.2 14.2 101.26 4.92 4.23 68.5 yes 2023 14 3.58 24.96 25.83 23.3 73.88 0.23 13.9 101.3 5.05 3.78 59.56 yes 2023 15 3.04 24.94 26.52 23.3 67.19 0.11 12.9 101.34 4.11 3.38 72.81 yes 2023 16 3.17 24.84 25.84 23.3 75.31 0 14.2 101.33 4.66 3.12 64.44 no 2023 17 3.07 25.16 26.05 23 80.31 0 15.1 101.3 4.05 3.09 56.94 no 2023 18 3.07 25.15 26.28 23.3 79.81 0.01 15.3 101.35 4.39 3.09 76.25 yes 2023 19 3.48 25.02 26.17 23.3 79.31 0 15.1 101.34 5 3.53 64.69 no 2023 20 3.77 25.04 26.02 23.1 81.12 0.01 15.3 101.32 5.2 4.06 65.62 yes Table 2 Descriptions of sample data. Weather parameters Detailed description Wind speed This refers to the speed at which air is moving horizontally past a certain point, typically measured in units in kilometers per hour (km/h) Earth skin Temp This is the Earth's surface temperature, often measured in degrees Celsius. It represents the heat energy present at ground level. Temp_Max This indicates the highest temperature recorded within a day, measured in degrees Celsius Temp_Min This indicates the lowest temperature recorded within a day, measured in degrees Celsius Relative Humidity Relative humidity is the amount of moisture in the air compared to the maximum amount of moisture the air can hold at that temperature. It's expressed as a percentage. Precipitation Precipitation refers to any form of water, such as rain, snow, sleet, or hail, which falls from the atmosphere to the Earth's surface. Measured in mm Specific humidity Specific humidity measures the amount of water vapor present in a parcel of air, typically expressed in grams of water vapor per kilogram of air. Surface pressure Surface pressure, also known as atmospheric pressure or barometric pressure, is the force exerted by the weight of air molecules above a certain point on the Earth's surface. It's usually measured in units like millibars (mb) Wind speed_Max Similar to Temp_Max, this is the highest wind speed recorded over a specific period, measured in units like meters per second (m/s) Wind speed_Min Similar to Temp_Min, this is the lowest wind speed recorded over a specific period, measured in units like meters per second (m/s) Rainfall Rainfall specifically refers to the amount of rain that has fallen over a specific period, typically measured in units like millimeters (mm) Table 3 Descriptive statistics. Statistic Year wind speed Earth skin Temp Temp_ Max Temp_ Min Relative Humidity Precipitation Specific humidity Surface pressure Wind speed_Max Wind speed_Min Wind direction count 15,340 15,340 15,340 15,340 15,340 15,340 15,340 15,340 15,340 15,340 15,340 15,340 mean 2002.4 4.0 28.6 30.3 26.3 75.1 2.7 17.7 100.8 6.4 3.2 149.5 std 12.1 1.0 1.7 2.3 1.5 6.1 6.2 1.7 0.3 1.5 1.6 76.9 min 1982.0 0.8 24.1 23.9 21.1 53.5 0.0 11.4 99.7 1.9 0.0 7.6 25 % 1992.0 3.3 27.2 28.1 25.2 70.8 0.1 16.7 100.5 5.2 1.9 67.3 50 % 2002.5 4. 28.9 30.4 26.5 74.9 0.6 18.0 100.7 6.3 3.1 163.0 75 % 2013.0 4.7 30.0 32.2 27.4 79.2 2.6 18.9 101.0 7.5 4.3 220.0 max 2023.0 10.2 33.1 36.3 30.3 93.5 135.1 22.2 101.6 16.5 11.0 347.2 Table 4a Specifications of hyperparameters. Sl.No Method Hyperparameter Values 1 RF n_estimatorsmax_depthmin_samples_leaf 69564 2 DT min_samples_splitmax_depthmin_samples_leafmax_leaf_nodes 2561030 3 SVM Regularization parameter (C)epsilonKernel typeRBF gamma 4000.005RBF0.010 4 LightGBM n_estimatorsmax_depthlearning_rate 10040.056 Table 4b Evaluation Metrics for classification and regression task. Metric Formula Root mean square error (RMSE) 1 N ∑ i = 1 M o i - p i 2 Mean absolute percentage error (MAPE) 1 N ∑ i = 1 M o i - p i o i × 100 % Mean absolute error (MAE) 1 M ∑ 1 M o i - p i Accuracy TP + T N TP + F P + F N + T N Specificity Recall TN FP + T N TP TP + F N Precision F1 Score TP TP + F P 2 ∗ ( p r e c i s i o n ∗ r e c a l l ) / ( p r e c i s i o n + r e c a l l ) Table 5a Before using VSHD (Random forest) Annual Rainfall in mm. Year Observed Predicted 2000 972 945 2001 483.4 579 2002 318 514 2003 348.5 842 2004 1037.6 1123 2005 1365.3 962 2006 927.9 825 2007 900 889 2008 1203.8 987 2009 928.5 862 2010 1095.2 982 2011 926.5 871 2012 636.5 792 2013 407 598 2014 500 567 2015 1400.6 985 2016 680 432 2017 973 762 2018 926 782 2019 988 822 2020 976 543 2021 1027 981 2022 982 654 2023 997 765 Table 5b After using VSHD (Random forest) Annual Rainfall in mm. Year Observed Predicted 2000 972 953 2001 483.4 463.4 2002 318 328 2003 348.5 358.5 2004 1037.6 1045.6 2005 1365.3 1385.3 2006 927.9 947.9 2007 900 880 2008 1203.8 1198 2009 928.5 910 2010 1095.2 1066 2011 926.5 914 2012 636.5 620 2013 407 447 2014 500 529 2015 1400.6 982 2016 680 690 2017 973 982 2018 926 936 2019 988 967 2020 976 1002 2021 1027 988 2022 982 1182 2023 997 890 Table 5c Before using Feature selection (RFECV) Annual Rainfall in mm. Year Observed Predicted 2000 972 959 2001 483.4 579 2002 318 414 2003 348.5 642 2004 1037.6 1123 2005 1365.3 1462 2006 927.9 1125 2007 900 707 2008 1203.8 900 2009 928.5 1129 2010 1095.2 892 2011 926.5 724 2012 636.5 930 2013 407 706 2014 500 207 2015 1400.6 802 2016 680 978 2017 973 477 2018 926 329 2019 988 885 2020 976 968 2021 1027 787.5 2022 982 867 2023 997 713 Table 5d After using Feature selection (RFECV) Annual Rainfall in mm. Year Observed Predicted 2000 972 969 2001 483.4 479 2002 318 314 2003 348.5 342 2004 1037.6 1023 2005 1365.3 1362 2006 927.9 925 2007 900 907 2008 1203.8 1200 2009 928.5 929 2010 1095.2 1100 2011 926.5 924 2012 636.5 630 2013 407 406 2014 500 507 2015 1400.6 1402 2016 680 678 2017 973 977 2018 926 929 2019 0988 985 2020 976 978 2021 1027 1023.5 2022 982 983 2023 997 994 Table 6 Comparison of various imputation method. Imputation Method RMSE MAE Mean 0.37 0.29 Median 0.32 0.21 Multiple Imputation by Chained Equations 0.46 0.27 Hot deck 0.49 0.22 VSHD 0.23 0.12 Table 7a Accuracy Metrics of different Base learners. Base learners Accuracy Kappa coefficient F1-Score DT 0.87 0.74 0.78 RF 0.88 0.76 0.89 LGBM 0.78 0.87 0.74 SVM 0.86 0.79 0.87 SHEM 0.96 0.97 0.94 Table 7b Error Metrics of different Base learners. Base learners R-squared Root Mean Squared Error DT 0.473 0.679 RF 0.752 0.834 LGBM 0.698 0.673 SVM 0.436 0.872 SHEM 0.952 0.321 An integrated framework for rainfall prediction and analysis using a Stacked Heterogeneous Ensemble Model (SHEM) P. Umamaheswari Project administration Investigation Resources Writing \u2013 original draft Data curation Visualization Supervision Conceptualization \u204e V. Ramaswamy Methodology Formal analysis Investigation Data curation Software Validation Formal analysis Visualization Writing \u2013 review & editing Computer Science and Engineering, SASTRA Deemed University, SRC Campus, Kumbakonam, Tamil Nadu 612001, India Computer Science and Engineering SASTRA Deemed University SRC Campus Kumbakonam Tamil Nadu 612001 India Computer Science and Engineering, SASTRA Deemed University, SRC Campus, Kumbakonam, Tamil Nadu 612001, India \u204e Corresponding author. Graphical abstract Recent climate conditions in India have led to numerous disasters, including floods, overflowing rivers, broken dams, and reduced vegetation. Machine learning aims to train models to make predictions based on historical data. Although many studies have focused on predictions and model building, this research introduces a novel integrated framework using a Stacked Heterogeneous Ensemble Model (SHEM). It aims to enhance prediction accuracy by employing another novel approach for imputing missing values, the Variable Specific Hot Deck (VSHD) imputation method. The outcomes were contrasted with established machine learning techniques, including random forest, decision tree, k-nearest neighbor, and support vector machine. After completing the imputation, we proceeded to implement the SHEM model. Real-time climate data for the Cuddalore location in Tamil Nadu state, South India was collected from the NASA Power Access viewer portal to verify the accuracy level of the proposed model. Performance analysis indicates that the proposed imputation outperforms all four alternative models with a 30% average improvement in accuracy. Moreover, the developed SHEM model has a reduced RMSE value of 0.321 and an R-squared value of 0.952 and shows a 9% improvement in accuracy compared to the base model\u2019s performance. Furthermore, these prediction results are compared to an LSTM (Long Short Term Memory), a deep learning model to calculate accuracy and loss, showing that the proposed achieves high accuracy and significant loss during validation. The obtained results can serve as a guideline for atmospheric scientists and various weather forecasting applications, helping them to choose the most appropriate machine learning method for their prediction task. Keywords Rainfall prediction Weather forecasting Imputation Machine learning Deep learning Regression Stacking ensemble model Data availability The dataset used in this study was obtained through NASA POWER Access Viewer, which provides a variety of climate, weather and solar energy data - https://power.larc.nasa.gov/data-access-viewer/ as described in the article. 1 Introduction In recent years, the unpredictability of climate conditions, especially in regions like India, has underscored the significance of accurate and timely rainfall predictions. Such forecasts aid in averting potential disasters like floods and dam breaches and ensuring agricultural sustainability and urban planning. Rainfall, an essential component of the Earth's water cycle, is vital for sustaining life, agriculture, and the overall health of ecosystems. Understanding rainfall patterns, variability, and potential predictability is crucial for numerous applications, ranging from agriculture and hydrology to urban planning and disaster management. Enter the concept of time series rainfall data. A time series is a collection of data points documented regularly throughout a defined timeframe. Regarding rainfall, this typically means collecting precipitation amounts at consistent intervals, such as hourly, daily, or monthly, over years or even centuries. This continuous data stream allows researchers and scientists to observe long-term trends, seasonal variations, and shorter-term fluctuations in precipitation patterns. Analysing time series rainfall data is not just about counting millimeters or inches of rain. It's about detecting underlying patterns, understanding cyclic behaviours like El Nino and La Nina, and forecasting future rainfall events. Spatial interpolation techniques generate a precipitation field with spatial distribution from precipitation data collected at specific points. (Zhang et al., 2020) Furthermore, an additional array of spatial interpolation approaches, including Geographically Weighted Regression (GWR), Geographically Weighted Regression Kriging (GWRK) incorporating the spatial correlation structure of residuals, and Geographically Temporally Weighted Regression (GTWR), are employed to clarify the dynamic spatiotemporal relationship between precipitation and elevation. In the era of climate change, where weather patterns are undergoing significant shifts, deeply understanding past and present rainfall data becomes paramount. This helps prepare for droughts, manage water resources, predict floods, and ensure agricultural productivity. With advancements in technology, the granularity and accuracy of time series rainfall data have improved significantly. Diverse methodologies have been applied in the literature, encompassing techniques such as Inverse Distance Weighted (IDW), Ordinary Kriging (OK), splines, Bayesian Maximum Entropy (BME), and multivariate geostatistical methods like cokriging, kriging with external drift, and regression kriging. A study has proposed a stack-based ensemble classifier (Hosseini Chagahi et al., 2024) for detecting cardiovascular diseases, focusing on applying machine learning techniques. The classifier uses an aggregation layer, dependent ordered weighted averaging (DOWA) operator, and feature transformation to identify heart and blood vessel anomalies early on. The study demonstrates that adding an aggregation layer to the stacking classifier significantly improves classification accuracy, resulting in % overall accuracy of 94.05 %. The aggregation layer also increases the area under the receiver operating characteristic (ROC) curve, reaching 97.14 %, further reinforcing the classifier's reliability and effectiveness in classifying cardiovascular disease. The study highlights the importance of early detection and intervention in reducing cardiovascular disease incidence, severity, and progression and preventing premature death. The study emphasizes the need for early intervention and the potential of this classifier to reduce the economic burden of cardiovascular diseases and improve quality of life. Another study (Da Silva et al., 2022) evaluates an ensemble learning model that combines bagging and stacking methods for time series forecasting of wind power generation. The model is applied to short-term and long-term evaluations of wind power generation, using arithmetic and weighted average values to integrate samples from the bagging strategy. The weights are defined through multi-objective optimization using a non-dominated sorting genetic algorithm \u2013 version II. The model was extensively tested using measurement data from two wind farms in Bahia State, Brazil. The experimental results show that the proposed ensemble learning model performs better than single forecasting models, such as stacking, machine learning, artificial neural networks, and statistical models. Results with a weighted average are 87.5 % superior to those with an arithmetic average for out-of-sample wind power forecasting in the evaluated forecasting horizons. The findings show that combining ensemble strategies can provide accurate forecasting results in the renewable energy. Researchers from the Odisha University of Technology and Research, Parala Maharaja Engineering College, Umea University, and the University Research and Innovation Center (EKIK) have developed a predictive model for heart disease prediction using stacking classifiers. (Mohapatra et al., 2023) The model is based on stacking various classifiers in two levels (Base level and Meta level) and combines various heterogeneous learners to produce strong model outcomes. The model achieved 92 % accuracy in prediction with a precision score of 92.6 %, sensitivity of 92.6 %, and specificity of 91 %. The model's performance was evaluated using various metrics, including accuracy, precision, recall, F1-scores, and area under the ROC curve values. The study highlights the importance of leveraging available clinical data in healthcare to improve patient outcomes. The researchers argue that integrating machine learning techniques with data analytics can lead to more efficient and accurate diagnosis of cardiovascular diseases. A multi-step short-term wind speed forecasting method based on multi-stage decomposition coupled with stacking-ensemble learning approach is developed by another team and wind energy is an emerging renewable energy source in Brazil, accounting for 17 % of the National Interconnected Network. To accurately predict wind energy, utilizing stacking-ensemble learning. (Ribeiro et al., 2022) the researchers propose an ensemble learning model that incorporates variational mode decomposition and singular spectrum analysis across multiple stages. Using a multi-step-ahead forecasting strategy, the model is tested and applied to short-term wind speed data from a wind farm in Parazinho, Brazil. The selected models for forecasting were partial least squares regression, k-nearest neighbors, cubist regression, support vector regression, and ridge regression. The results showed an average performance improvement between 3.71 % and 21.38 % compared to dual decomposed models, 37.18 and 52.47 percent compared to single decomposition, and 54.98 % better results than models without decomposition. The dual decomposition ensemble learning model is an effective and accurate approach for forecasting wind speed. Authors from Odisha University of Technology and Research have identified(Luo et al., 2024) Major depressive disorder (MDD) as a serious and heterogeneous psychiatric disorder. A machine learning method (MFMC) for MDD discrimination is proposed by concatenating multiple features and stacking multiple classifiers. MFMC is tested on the REST-meta-MDD data set, which contains 2428 subjects collected from 25 sites. MFMC yields 96.9 % MDD discrimination accuracy, demonstrating a significant improvement over existing methods. The generalizability of MFMC is validated by the good performance when training and testing subjects are from independent sites. The use of XGBoost as the meta-classifier allows the researchers to probe MFMC's decision-making process. They identify 13 feature values related to 9 brain regions, which contribute most to the classification and demonstrate significant differences at the group level. These features may serve as clinically useful diagnostic and prognostic biomarkers for MDD in the future. Modern instruments and satellite measurements complement traditional rain gauges, offering a more comprehensive picture of rainfall patterns across large regions. The challenge and opportunity lie in leveraging this rich dataset and utilizing statistical and machine learning methods to better predict and respond to the changing dynamics of rainfall. Intricate patterns and data sequences that require sophisticated analytical tools to decipher are at the heart of these predictions. Enter the realm of time series analysis, a powerful statistical method that can capture and predict temporal shifts in climate data. With the advent of machine learning, the landscape of rainfall prediction has been revolutionized. The SVM method has proven to be a robust and efficient prediction model, especially compared to the Monte Carlo approach. Traditional statistical methods, such as linear and logistic regression (Evans et al., 2020), have been foundational tools in atmospheric science for decades. However, the complexity of current climate dynamics demands a more comprehensive and nuanced approach. This is where techniques like the Stacked Heterogeneous Ensemble Model (SHEM) come into play. By combining the strengths of various machine learning models, SHEM offers the promise of reduced overfitting and enhanced prediction accuracy. In the face of escalating challenges climate change presents, creating and implementing sophisticated forecasting tools are of utmost importance for the global community. This research delves into the intricacies of machine learning-driven rainfall prediction, compares traditional regression techniques with the cutting-edge SHEM, and evaluates their efficacy in Indian climate conditions. Flood condition maps are produced using GIS, leveraging rainfall data from various periods. Drawing upon the regression model feature, the importance of conditioning factors was determined, as highlighted (Wu et al., 2020). Upon analysing the rainfall return period in Zhengzhou City, notable increases were observed in the quantity and depth of water accumulations. The effectiveness and reliability of the method were assessed by forecasting urban flood depths, resulting in a demonstrated accuracy of 11.52 %. The study's conclusions establish a scientific basis for urban flood control and drainage planning. To validate the developed models, various quality assessment criteria were employed, including correlation coefficient (R), mean absolute error (MAE), skill score (SS), probability of detection (POD), critical success index (CSI), and false alarm ratio (FAR). Except for SVM, all AI models provided daily rainfall predictions, and among them, SVM emerged as the superior method for rainfall prediction. The structure of this paper is outlined as follows: Section 2 provides an overview of related work, whereas Section 3 introduces the proposed system framework. Implementation results and the evaluation of the proposed system are detailed in Section 4. Section 5 concludes the paper and offers suggestions for future work. 2 Related work In this section, various relevant studies and works have been deliberated upon. Forecasting rainfall holds significant importance in meteorology and environmental science, playing a crucial role in agriculture, the management of water resources, and disaster preparedness. This literature survey explores key studies and advancements in rainfall prediction, focusing on various methodologies and technologies employed for accurate forecasting. Forecasts tend to increase in autumn, peak during winter rainfall, and decrease towards the end of spring. The period from May to October represents an optimal season for forecasting, with three months in most years exhibiting better skill than climatology. A sample of six years of forecasts was analysed to determine the best or worst accuracy. If there's a risk associated with accuracy, it's advisable to opt for an approach that offers minimal risk but better accuracy. The M5Tree model exhibits superior accuracy compared to traditional statistical methods among machine learning models like ANFIS-PSO, ANFIS-FCM, and MARS(Adnan et al., 2021). Combining ANFIS-FCM, MARS, M5Tree, and ANFIS-PSO models with the MM-SA ensemble model enhances accuracy, improving 8.5 %, 7.4 %, and 28.8 % in RMSE, respectively. In general, machine learning methods outperform EBA4SUB, although there are instances where EBA4SUB achieves better accuracy than both M5Tree and MARS. An analysis comparing the advantages and disadvantages of DLWP against NWP is presented (Ren et al., 2021). Agglomerative clustering and the ANOVA approach (Li, 2021) to accurately identify sensor locations for detecting drainage flooding. The number and placement of sensors impact rainfall uncertainty, with effects observed over the period from 2000 to 2009. These findings are particularly relevant in urban drainage catchments, where extreme rainfall events and climate variations hold significant implications for decision-makers and engineers. In an endeavour to assess environmental deterioration stemming from raindrop effects and related factors, this study by (Ridwan et al., 2021) the researcher utilizes the adaptive capabilities of genetic programming (GP) for forecasting soil erosion. The predictive model integrates parameters such as rainfall volume, kinetic energy, rainfall intensity, gully head advancement, soil detachment, factored soil detachment, runoff, and runoff rate, derived from a three-year comprehensive dataset. They employed two methods for forecasting rainfall. The first method utilized the autocorrelation function, while the second relied on forecasting rainfall using projected error. After parameter tuning, the first method exhibited the highest coefficient of determination, signalling superior performance for each scenario. When analysing the second method and comparing it to other model performances, normalization using the log-normal distribution yielded favourable results across each category. Specifically, BDTR and DFR outperformed both NNR and BLR. This study explored two methods under varied scenarios and time horizons. Ultimately, the first method demonstrated higher accuracy relative to the second. The assessment of a model's performance using metrics (Barrera-Animas et al., 2022) such as root mean squared error, mean absolute error, and root mean squared logarithmic error. By evaluating different networks, they identified that the staggered LSTM network with two hidden layers and the bidirectional LSTM network exhibited the most effective performance. Notably, LSTM networks with fewer hidden layers demonstrated superior performance compared to their counterparts. This approach was specifically applied to rainfall forecasting applications with budget constraints. Four supervised machine learning approaches, including decision tree, naive Bayes, k-nearest neighbors, and support vector machines. (Rahman et al., 2022) Their study utilized a 12-year dataset of historical weather information covering the period from 2005 to 2017 in Lahore. Before deploying these classification techniques, the dataset underwent a cleaning and normalization process. Their results indicated that a fusion-based output yielded superior results to other models. (Ridwan et al., 2021)investigated the utilization of artificial intelligence alongside nature-inspired algorithms like biomimetic or metaheuristics to tackle complex issues in geotechnical and civil engineering. This approach provides advantages for the ecosystem and facilitates substantial progress towards achieving zero carbon footprints in geo-environmental engineering. Some of the research group collected rainfall images from a dense array of sensors, employing a convolutional neural network (CNN) model dubbed the image-based rainfall CNN (irCNN) to gauge rainfall intensity values. This irCNN model (Yin et al., 2023)estimated rainfall intensity from sensor-generated images. By leveraging both synthetic rainfall data and real rainfall images, the researchers could assess the irCNN model's accuracy theoretically and empirically. Their findings indicated that the mean absolute percentage error of the irCNN model ranged between 13.5 % and 21.9 %. The irCNN offers an economical solution for generating high-spatiotemporal urban rainfall data. Such data, when assessed in real-time, can significantly bolster urban flood risk management strategies, particularly when forecasting urban rainfall intensity via the irCNN model. A convolutional neural network (CNN) (Rampal et al., 2022) architecture based on multiple linear regression with manual feature selection. This approach outperformed traditional statistical downscaling models. Throughout the examined region, there was an increase in wet days from 0.35 to 0.52 based on variance, and the root mean squared error decreased by over 20 %. Furthermore, rainfall accuracy improved in the 90th percentile by 25 %. One of the notable capabilities of machine learning methods is their ability to self-learn the relationship between the large-scale atmospheric environment and extreme localized rainfall events. Authors 2022 observed (Rahman et al., 2022) nearly exponential growth in research activity within this domain, highlighting Artificial Neural Networks (ANN) and its modifications as the predominant artificial intelligence technique, constituting approximately half of the research efforts. Additionally, they noted that the correlation between soil and rock properties emerges as the primary focus, encompassing around 30 % of the research endeavours. This capability endorses the broader development and application of deep learning in climate downscaling, especially given its proven historical performance and physical interpretability. Some of the researchers observed that (Fahad et al., 2023) in comparison to state-of-the-art rainfall forecasting models, metrics such as the normalized root mean squared error (NRMSE) and normalized mean absolute error (NAME) retained high predictive accuracy. Correlation and regression analyses indicate the importance of climatic variables in forecasting. The findings underscore a positive correlation between air quality and rainfall every quarter of the year. Conversely, a negative association was observed with temperature when employing correlation techniques. In the initial and second quarters of the year, there is no discernible connection between air quality variables. Climatic variables with strong associations provide further insights into rainfall patterns. In their proposed model, (Barrera-Animas et al., 2022) validated five years of rainfall data in Taiwanese provinces using meteorological parameters. Internal and external validation experiments were conducted for slight, moderate, and heavy rainfall cases. The proposed High-Resolution Rainfall Forecast (HRF) model yielded an average root-mean-squared error (RMSE) and relative RMSE of 1.36/1.39 mm/h and 1.00/0.67, respectively. Comparative studies have demonstrated the HRF model's superiority in time resolution and forecasting performance. In the case of rainfall, an ensemble of LSTM models, as presented by (Jose et al., 2022), achieved superior accuracy with a coefficient of determination value of 0.9 compared to other models. Random Forest (RF) and LSTM consistently performed well in temperature-related cases, while other machine learning methods exhibited equally impressive results. Consequently, RF and LSTM methods are the foundation of Multi-Model Ensembles (MMEs). Ultimately, the mean ensemble method outperformed the all-machine learning approach. Collected datasets (Ko et al., 2022)over seven years from South Korea, utilizing radar images and precipitation data. The Critical Success Index (CSI) demonstrates a marked improvement with pre-training and incorporating a new loss function for heavy rainfall predictions, showcasing increases of up to 95.7 % and 43.6 % for a five-hour lead time. In comparison to conventional methods, our approach manages to reduce the precipitation estimation error by as much as 10.7 %. Additionally, we analysed four heavy rainfall cases with varying resolutions. The DWT-CLSTM-DCCNN training model utilizes two distinct methods of sample construction (Wei and You, 2022). Both LSTM and DCCNN were constructed as benchmark models. The precipitation data from four main Chinese towns were assessed using the DWT-CLSTM-DCNN model. In contrast to other benchmark models, specifically mean absolute error (MAE), root mean squared error (RMSE), and Nash-Sutcliffe efficiency (NSE), DWT-CLSTM-DCNN displays praiseworthy forecasting curves. This model effectively captures mutational rainfall trends and showcases heightened accuracy. In the research by (Ouma et al., 2021) five climatological parameters were evaluated using LSTM and WNN models. The performance metrics, particularly RR values, were found to be 0.8967 for LSTM and 0.8820 for WNN. The LSTM outclassed the WNN in monthly rainfall predictions, achieving R2 = 0.8610 compared to WNN's R2 = 0.7825 when using satellite-based meteorological data. For monthly rainfall predictions, the RMSE varied between 15 and 21 mm, while the MAE ranged between 9 and 11 mm. Performance enhancement of the models was observed with the increase in input parameters. While both models converged to the lowest RMSE, WNN required a longer computational time to achieve its maximum RMSE. This study underscores the effectiveness of deep-learning neural network models in utilizing satellite-based meteorological data for precise rainfall analyses. Their proposed model, (Danandeh Mehr et al., 2023) assessed the efficiency of various techniques, including CNN, LSTM, artificial neural networks, and genetic programming. Remarkably, the CNN-LSTM model demonstrated superior performance compared to all benchmarks. Testing was conducted at the Nallihan and Beypazari stations for SPEI-3 and SPEI-6 indices. Quantitative visualizations revealed RMSE values of 0.73 and 0.53 for Nallihan and 0.75 and 0.43 for Beypazari. This prediction model for SPEI contributes to our deeper understanding of meteorological drought patterns. A neural network model capable of predicting precipitation with high resolution for up to 12 h (Espeholt et al., 2022). This new model competes favorably with the current state-of-the-art physics-based models in predicting raw precipitation for similar durations for the continental United States. Their findings mark the introduction of a novel class of neural weather models. (Panda et al., 2024) Researchers in 2024 explored the potential for improved results in rainfall analysis and forecasting, suggesting that utilizing a blend of Bi −LSTM and GRU could enhance outcomes within the univariate approach. Similarly, Panda proposed that employing a more sophisticated iteration of LSTM might enhance results within the multivariate approach. The LSTM model forecasts that India will likely experience rainfall of approximately 790 mm in 2023, which falls close to the boundary between normal and below-normal categories. Hence, it is advisable to take cautious measures to mitigate any adverse effects of a below-normal monsoon season. The simultaneous occurrence of El Niño and the Indian Ocean Dipole (IOD) appears to counteract their individual effects, resulting in a normal rainfall pattern for the upcoming season due to atmospheric interactions, akin to observations in the significant El Niño event of 1997\u20131998. These findings by the authors (Narang et al., 2024) underscore the crucial role of inter-basin connections between the Pacific and Indian Oceans in predicting seasonal rainfall in the Asian monsoon region, highlighting the significance of this linkage as a source of predictability. This aligns with ongoing discussions in the scientific community that stress the importance of understanding the intricate interactions between these two oceanic basins and their influence on weather and climate patterns. Another research team (Peñalvo et al., 2022)conducted a thorough comparative analysis, assessing various machine learning models' performance in predicting stock prices. After careful experimentation and analysis, Fb-prophet stands out as the favoured option for accurate prediction within the scope of their investigation. The major goal of this study is to predict the rainfall and irrigation needs of corn crops in various Ecuadorian locations. Corn is a staple food that is essential to the national economy of Ecuador and the other Andean nations. The methodology uses regression models within the context of Functional Data Analysis (FDA) to improve the quality of the maize harvest. The model estimates the amount of rainfall in the Ecuadorian locations with the highest corn output using functional factors like temperature and wind speed. The value of real evapotranspiration, which is the percentage of water used by the crops, and, more crucially, the irrigation needs for the corn crop at each stage of growth are obtained from the estimation of rainfall, which is the amount of water used by the crops. After applying the model to locations in North Peru (Flores et al., 2023) rainfall forecast errors ranged from 9 % to 22 %. The study also suggests deploying FDA outlier identification and exploratory analysis approaches as a typical and helpful practice in the field of rainfall prediction studies. Artificial intelligence (AI) and machine learning (ML) advancements have made it possible to predict ocean and atmospheric variations more accurately. Since these techniques can successfully spot emerging trends in massive volumes of observational and model data, they are better suited for forecasting in a range of industries, including agriculture, energy, water (Narang et al., 2024) resources, health, and disaster management. The best-performing AI model predicts that 2023 the AISMR for a typical monsoon year will be approximately 790 mm. Accurate forecasting is crucial for disaster management, energy, agriculture, health, and water resources, among other fields, for the purpose of creating policies and making choices. However, traditional physical models suffer from several issues, such as high computing costs, a strong sensitivity to parameter initialization, and limited input flexibility. Accurate forecasts are necessary in many domains, such as disaster management, energy, agriculture, health, and water resources, to formulate policies and make choices. However, traditional physical models suffer from several issues, such as high computational cost, severe sensitivity to parameter initialization, and limited input parameter flexibility. It has been demonstrated that ML and AI are effective techniques for predicting the ocean and atmosphere. 3 Proposed model As previously stated, it's a formidable technique in machine learning. SHEM is an ensemble machine learning technique that amalgamates predictions from diverse models to generate a conclusive prediction, thereby improving the overall predictive performance. It incorporates a variety of base learners, which can encompass entirely different algorithms, including Decision Trees, Random Forests, Light GBM (Gradient Boosting Machines), and SVMs (Support Vector Machines); the term \u201cheterogeneous\u201d emphasizes the diversity among the base learners. Each base learner within the ensemble can utilize a distinct learning algorithm, varying in complexity, assumptions, and underlying mechanisms. For Instance, Decision Trees are known for their simplicity and interpretability, while Random Forests introduce randomness and bagging to improve performance. As shown in Fig. 1 , the Stacked Heterogeneous Ensemble Model (SHEM) distinguishes itself from homogeneous ensemble techniques by employing diverse base learners. While homogeneous ensemble techniques, such as Random Forests, typically utilize multiple instances of the same model (e.g., decision trees), SHEM takes a different approach. Light GBM employs gradient boosting for better accuracy, and SVMs focus on finding optimal hyperplanes for classification. By incorporating a mix of base learners with different strengths and weaknesses, SHEM can harness the collective intelligence of various learning algorithms. Each base learner brings a unique perspective to the task at hand, capturing different aspects of the data and contributing diverse insights. This diversity helps mitigate the risk of overfitting to specific patterns in the data and enhances the model's robustness and generalization ability. 3.1 Study area and dataset The dataset utilized in this study was obtained from NASA's Power Access Viewer, a comprehensive platform providing access to relevant environmental and atmospheric data. The data collection process involved accessing various sources within the Power Access Viewer, ensuring the inclusion of pertinent variables necessary for the analysis. The dataset utilized for this proposed work spans from 1982 to 2023, encompassing a comprehensive range of data over a forty-one-year period. This dataset constitutes a valuable resource for investigating the dynamics of weather patterns and atmospheric conditions, aligning with their research objectives. These measures were undertaken specifically concerning Cuddalore. It is a district located in the southern state of Tamil Nadu, India. Cuddalore is located on the southeastern coast of India, with its geographical coordinates approximately at a latitude of 11.7447° N and a longitude of 79.7688° E. There are 7 talukas in Cuddalore district. The Bay of Bengal bounds it to the east. It shares its boundaries with the Viluppuram district in the west and north, the Bay of Bengal in the east, and the Nagapattinam district in the south, and it has been depicted in Fig. 2 . Being a coastal district, Cuddalore is prone to cyclones, sea-level rise, and other climate-related challenges. This vulnerability offers a ground for studies focused on disaster preparedness, community resilience, and climate change mitigation strategies. The district's coastal areas are also vulnerable to cyclones, rising sea levels, and other climate change impacts. Various parameters are used to quantify and describe specific aspects of the atmosphere and its behaviour. Table 1 exhibits sample parameters collected in the year 2023, representing the dataset's features for that particular timeframe and lists some of these crucial parameters, each with a distinct role in shaping our understanding of weather conditions. These parameters encompass a wide range of meteorological variables such as wind speed, temperature, humidity, precipitation, surface pressure, and wind direction. Table 2 furnished a detailed description of the dataset. Over the years, Cuddalore has been a hotspot for environmental concerns, especially due to its growing industrial belt. Environmentalists and locals have frequently raised issues related to industrial pollution. Table 3 presents a set of descriptive statistics commonly used to summarize numerical data distributions. The \u201cCount\u201d column indicates the total number of observations or data points in the dataset, providing an overview of its size. The mean value represents the average of all observations, measuring central tendency. Standard deviation (SD) quantifies the dispersion of values around the mean, indicating the variability within the dataset. The \u201cMin\u201d and \u201cMax\u201d values denote the smallest and largest observations, respectively, providing insights into the present values range. Quartiles, represented by the 25th percentile (Q1), 50th percentile (Q2 or median), and 75th percentile (Q3), partition the data into four equal parts, offering information on the spread and distribution of values. Together, these statistics offer a comprehensive summary of the dataset's characteristics, aiding in understanding its distributional properties and informing further analysis. 3.2 Imputing missing values by Variable Specific Hot Deck (VSHD) Variable Specific Hot Deck (VSHD) is an approach for filling in missing values, wherein each missing value is substituted with a perceived response from a unit considered to be similar. It's like picking a value from a \u201cdeck\u201d of observed values comparable to the missing one. Here's how it can be applied to missing values in rainfall prediction. When predicting rainfall, several factors or variables influence the predictions, such as temperature, humidity, wind patterns, historical data, etc. If some of these values are missing, it can significantly impact the accuracy of the prediction. VSHD can fill in these gaps by borrowing data from similar time points or geographical locations where the data is not missing. Variable Specific Hot Deck (VSHD) imputation is used to fill missing values in a dataset by replacing them with values from similar cases within the same variable. Steps_Variable Specific Hot Deck (VSHD) Initialize the RF_SET with missing values For each Instance in RF_SET (i) Create an empty list: hot_deck (ii) For each Instance in the dataset Compute a similarity measure between the values for the relevant variables and the Instance being imputed. (i) Add the current Instance to the hot_deck list along with its similarity measure. (ii) Sort the hot_deck list based on the similarity measure in ascending order Create \u201cimputation classes\u201d based on the sorted dataset For each variable in the current Instance (i) If the variable has a missing value, replace it with the corresponding value from the Sorted hot_deck Repeat steps 3 and 5 for all units with missing values until all missing values are imputed. (1) Y i m p = ∑ i = 1 n w i · Y neigh i ∑ i = 1 n w i Eq. (1) presents the formula for implementing VSHD imputation. Where n is the number of similar cases Xneigh, wi represents the weight assigned to each similar case based on its similarity to the missing case. Yneighi is the observed value of variable Y for a similar case. The weights wi can be determined using various similarity measures such as Euclidean distance, cosine similarity, or other distance metrics calculated based on variables X. Eq. (1) calculates the imputed value Yimp by taking a weighted average of the observed values of variable Y for similar cases Xneigh, where the weights are determined based on the similarity of these cases to the missing case. Implementing this formula requires determining the appropriate similarity measure and weighting scheme based on the specific characteristics of the dataset and the variables involved. 3.3 Feature selection Recursive Feature Elimination with Cross-Validation (RFECV) is a feature selection technique commonly used in machine learning tasks, including rainfall prediction. Here's the mathematical model for RFECV adapted for rainfall prediction feature extraction. The components within this feature selection technique include the following variables . X represents the input dataset containing meteorological parameters such as temperature, humidity, wind speed, etc., recorded at various geographical locations and time intervals. Y represents the corresponding observed rainfall data and n is the total number of features in the dataset. k is the number of features to select through the RFECV process. CV is the number of cross-validation folds used in the RFECV process. Steps_RFECV Process (i)Initialization Start with all n features included. (ii) Cross-validation and Model Fitting Split the dataset X and Y into CV folds. For each fold Train a rainfall prediction model using the subset of features. Evaluate the performance of the model using a Mean Squared Error) on the validation (iii)Feature Ranking: Calculate the average performance across all folds for each feature. Rank the features based on their average performance. (iv)Feature Elimination Eliminate the least important feature(s) from the dataset. Update the dataset with the remaining features. (v)Stopping Criterion If the desired number of features k is reached or if stopping criteria are met, stop the process. Otherwise, repeat steps 2\u20134. This algorithm iteratively selects the most important features based on cross-validation performance until the desired number of features, k, is reached or stopping criteria are met. Xi represents the dataset with i-selected features. Mi represents the trained rainfall prediction model using Xi. Ei represents the evaluation metric (MSE) of Mi using cross-validation. Algorithm. 1 RFE_Cross validation Start X0 = X i = 0 while i < k and stopping criteria are not met For each feature j in Xi: Train Mi using Xi with feature j removed. Calculate Ej using cross-validation. Select the feature j* with the lowest Ei. Xi+1 = Xi with feature j* removed i = i + 1 End 3.4 The Stacked Heterogeneous Ensemble Model (SHEM) Several machine learning methods are combined in the Integrated Framework for Rainfall Prediction and Analysis using a Stacked Heterogeneous Ensemble Model (SHEM) to improve accuracy. With the use of a stacking technique, SHEM creates a heterogeneous ensemble by layering several models, each of whose predictions serve as an input for a subsequent model. This method improves performance overall by utilizing the advantages of several models. The framework covers both classification (e.g., forecasting whether it will rain or not) and regression (e.g., forecasting the quantity of rainfall). The Integrated Framework for Rainfall Prediction and Analysis combines several machine learning methods for decision-making by combining a variety of data sources and complicated algorithms. As previously stated, it's a formidable technique in machine learning. SHEM is an ensemble machine learning method that combines predictions from multiple models to produce a final prediction, enhancing the overall predictive performance. Unlike homogeneous ensemble techniques that use multiple instances of the same model (e.g., Random Forests using many decision trees), SHEM employs a diverse set of base learners, which can be completely different algorithms like Decision Tree, Random forest, Light GBM and SVM. The predictions from the individual base learners are used as input features for a higher-level classifier, often called the meta-model or meta-learner. The heterogeneity among base learners allows the ensemble to capture different data patterns, trends, and nuances. Fig. 3 . illustrates that the proposed model derives its final prediction by assimilating input from the foundational models. Each model might excel in a particular data aspect, and SHEM leverages this diversity for improved accuracy. Stacking acts as a form of model regularization, reducing the likelihood of overfitting. By depending on multiple models rather than one, SHEM often demonstrates better generalization on unseen data. While individual models like decision trees or linear regression might offer straightforward interpretability, SHEM can be more challenging to interpret due to its layered structure. f ( · ) is the activation function of the meta-classifier (XGBoost), which can be a suitable function for rainfall prediction, such as a linear function customized activation function. (2) y ` ensemble x = f w D T · y ` D T ( x ) + w R F · y ` R F ( x ) + w L G B M · y ` L G B M ( x ) + w S V M · y ` S V M ( x ) + w meta · y ` meta ( x ) where f ( · ) is the activation function of the meta-classifier (XGBoost), which can be a suitable function for rainfall prediction w D T , w R F , w L G B M , and w S V M are the weights assigned to the predictions of the individual base learners (Decision Tree, Random Forest, Light GBM, and SVM). These weights can be learned during training using techniques like gradient descent or determined through cross-validation. w meta is the weight assigned to the prediction of the meta-classifier. Similarly, this weight can be learned during training or determined through cross-validation. 3.4.1 Computational complexity of proposed with traditional models For each base model, generating predictions has a complexity of O (N⋅d) for N data points with d features. Evaluation metrics are calculated for N data points, and the complexity of the evaluation would be O(N). Combining these complexities, the overall computational complexity of the SHEM can be approximated as: OSHEM=O (train) + O(N⋅d) + O (N⋅M). In random forest, the analysis would randomly select features at each node: O(M⋅N⋅d), where d is the number of features. 3.4.2 Specifications of hyperparameters for stacking ensemble models Four separate base models are used in this stacking ensemble setup: Random Forest (RF), Decision Tree (DT), Support Vector Machine (SVM), and LightGBM. For optimal performance, each model has a unique set of hyperparameters that must be adjusted. The hyperparameter specifications for stacking ensemble models are presented in Table 4a . The number of estimators (n_estimators), the maximum depth of the trees (max_depth), and the lowest number of samples needed to be at a leaf node (min_samples_leaf) are the hyperparameters for Random Forest. The least number of samples needed to divide an internal node (min_samples_split), the maximum tree depth (max_depth), the minimum number of samples needed to be at a leaf node (min_samples_leaf), and the maximum number of leaf nodes (max_leaf_nodes) are the hyperparameters for a decision tree. The SVM's four hyperparameters are the regularization parameter (C), epsilon, kernel type (RBF), and gamma. The last three hyperparameters for LightGBM are the learning rate, the max depth of the trees, and the number of estimators (n_estimators). The ensemble can minimize overfitting and effectively learn from the data to provide accurate predictions by adjusting these hyperparameters. An extensive grid search and a random search were actively carried out throughout predetermined ranges for every parameter to fine-tune the hyperparameters. For Instance, in the Random Forest and LightGBM models, values ranging from 100 to 1000 were investigated for n_estimators, from 2 to 30 for max_depth, and from 0.01 to 0.1 for learning_rate. Max_depth was varied from 6 to 10, min_samples_leaf from 4 to 30, max_leaf_nodes from 10 to 50, and min_samples_split from 2 to 30 in the Decision Tree model. The range of adjustments for C and epsilon in SVM was 0.1 to 1000 and 0.001 to 0.01 respectively, whereas the range for gamma was 0.005 to 0.010. These ranges were chosen from reference articles. Furthermore, the documentation given by the sci-kit-learn and LightGBM libraries was examined to verify compliance with known guidelines. By using this methodical approach, the chosen hyperparameters were rigorously optimized to improve the model's performance and generalizability. 3.5 Cross-Validation To ensure that the ensemble's performance is robust and reliable, it's beneficial to implement techniques like k-fold cross-validation within the SHEM framework. In conclusion, SHEM is a versatile and potent ensemble technique that aims to deliver superior predictive performance by integrating the strengths of diverse base learners. It's particularly effective in complex scenarios where no single model can capture all data intricacies. The Stacked Heterogeneous Ensemble Model (SHEM) combines multiple base classifiers using a meta-model, enabling the strengths of individual models to be harnessed for improved predictive accuracy. This approach leverages base model predictions as meta-model features, ensuring more generalization. In the supervised learning approach, X_train contains the input data or autonomous variables, and Y_train contains the equivalent labels or dependent variables. Together, they provide the foundational structure for any predictive model, offering a historical lens into the relationship between input features and the desired outcome. Cross-validation is a powerful technique in machine learning. It not only helps in estimating the performance of a model on hidden data but also reduces the risk of overfitting by using different subsections of the training data. In SHEM algorithm, the training data is divided into 'K' subsets, also known as folds. The dataset folded into multiple separate chunks. One chunk becomes the validation set for each fold iteration, while the rest serve as the training set. By rotating this validation chunk for each iteration, every data point can be in both training and validation subsets. The average performance across all iterations gives a more reliable measure of the model's capability. SHEM offers a robust method for capitalizing on the predictive capabilities of multiple models. By employing a meta-classifier to synthesize the outputs of base classifiers, SHEM can deliver improved generalization and accuracy, making it a promising approach for various prediction tasks. 4 Results and discussion Table 4b displays the performance metrics of four classifiers: K-Nearest Neighbors (KNN), Decision tree (DT), Support Vector Machine (SVM), and Random Forest (RF) evaluated in the context of imputing missing values using the Variable Specific Hot Deck (VSHD) method for rainfall prediction. Before applying, VSHD is calculated, and it has been presented in Table 1. For KNN, the model's accuracy is 82 %, meaning it correctly forecasted rainfall in 82 out of every 100 observations. Its precision stands at 66 %, which indicates that of all predictions labelled as rainy, 66 % were indeed associated with rain. A recall value of 51 % indicates that KNN accurately detected 51 % of the correct rainfall instances. Regression tasks employ RMSE, MAPE, and MAE to quantify prediction errors. For classification tasks, the model's effectiveness is assessed using accuracy, specificity, recall, precision, and F1 score as shown in Table 4b. 4.1 Experimental setup Two experiments were conducted: one without VSHD imputation (Table 5a ) and another incorporating VSHD with Random Forest (Table 5b ). Both tables present observed and predicted rainfall values for the years 2000 to 2023, facilitating a comparative analysis of model performance. Table 5a compares the predicted rainfall values using VSHD with Random Forest against the observed rainfall values before applying the imputation technique. The predicted values in Table 5a exhibit deviations from the observed values, indicating potential inaccuracies in the model predictions. For Instance, in some years (e.g., 2003, 2005, 2016), the predicted rainfall values deviate significantly from the observed values, suggesting room for improvement in the model's accuracy. Table 5b presents the predicted rainfall values after applying VSHD imputation with Random Forest. Compared to Table 5a, the predicted values in Table 5b demonstrate closer alignment with the observed values, indicating improved prediction accuracy. In several instances (e.g., 2001, 2002, 2003, 2006, 2007), the discrepancies between the observed and predicted rainfall values have reduced after incorporating VSHD imputation, suggesting enhanced model performance. The results indicate that applying Variable Specific Hot Deck (VSHD) imputation with Random Forest improves rainfall prediction accuracy. It has been represented in Fig. 4 a and 4b. By leveraging VSHD imputation techniques, the model produces predictions that closely match the observed rainfall values, offering promising implications for rainfall forecasting applications. Table 5c presents the observed and predicted rainfall values for each year from 2000 to 2023 before applying any feature selection technique. The observed column displays the actual measured rainfall values for each year. The Predicted column shows the rainfall values predicted by the model before feature selection. For Instance, in 2000, the observed rainfall was 972 units, while the model predicted 959 units. Similarly, in 2023, the observed rainfall was 997 units, and the model predicted 713 units. Table 5d displays the observed and predicted rainfall values for each year from 2000 to 2023 after applying feature selection. The \u201cObserved\u201d column represents the measured rainfall values, similar to Table 5a. The \u201cPredicted\u201d column presents the rainfall values predicted by the model after feature selection. For example, in 2000, the observed rainfall remained at 972 units, but the model's prediction improved to 969 units after feature selection. Similarly, in 2023, the observed rainfall was 997 units, and the model's prediction improved to 994 units after feature selection. Feature selection is a process used to select the most relevant features from the dataset, eliminating irrelevant or redundant features. Doing so aims to improve model performance, reduce overfitting, and enhance interpretability. Tables 5a and 5b demonstrate the impact of feature selection on the accuracy of rainfall prediction. After applying feature selection, the predicted rainfall values align more closely with the observed values, indicating improved model accuracy and performance. The improvement in prediction accuracy observed in Table 5c compared to Table 5d suggests that the selected features are more informative and relevant for predicting rainfall, leading to more accurate predictions. The results has been plotted in Fig. 4a and 4b. The F1-Score, calculated at 92 %, acts as a combined measure, capturing the balance between the model's precision and recall. Lastly, the specificity value of 55 % highlights the model's capability to appropriately identify 55 % of the cases without rain. In predicting rain, the Random Forest (RF) model demonstrates an accuracy rate of 79 % and has a precision of 63 %, a recall of 54 %, an F1-score of 93 %, and a specificity of 54 %. The Support Vector Machine surpasses KNN and RF in performance, attaining an accuracy of 87 %, a precision of 74 %, and a specificity of 63 %. Notably, its recall, at 52 %, is marginally greater than that of KNN, and the model possesses an F1-score of 84 %. Out of all the classifiers, the Radom Forest RF) stands out with the most commendable performance, boasting an accuracy of 91 %. Furthermore, RF has an impressive precision of 83 %, a recall of 54 %, an F1-score of 91 %, and a particularly robust specificity of 81 %. 4.2 Comparison of imputation methods Table 6 compares different imputation methods based on their performance measured by Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). The imputation methods evaluated include Mean, Median, and Multiple Imputation by Chained Equations (MICE), Hot deck, and VSHD (Variable Selection in Hot Deck). Mean imputation resulted in an RMSE of 0.37 and an MAE of 0.29, indicating moderate performance. Median imputation performed slightly better with an RMSE of 0.32 and an MAE of 0.21, showcasing its effectiveness in reducing errors. Multiple Imputation by Chained Equations (MICE) yielded an RMSE of 0.46 and an MAE of 0.27, indicating a higher error than simpler methods. Hot deck imputation demonstrated similar performance with an RMSE of 0.49 and an MAE of 0.22, suggesting its limited effectiveness in handling missing data. Notably, VSHD exhibited the lowest RMSE of 0.23 and the lowest MAE of 0.12, indicating superior performance among the evaluated methods. Overall, the table highlights the varying efficacy of different imputation techniques, with VSHD standing out as the most effective method in reducing errors. The comparison chart depicted in Fig. 5 . The discrepancy between a model's anticipated and actual values is measured by the Root Mean Square Error (RMSE). It is a valuable statistic for evaluating the accuracy of imputation techniques because of its heightened sensitivity to big errors. Among all the approaches tested, VSHD has the lowest root mean square error (RMSE) of 0.23. This suggests that compared to values imputed using other techniques, values imputed using VSHD are, on average, closer to the true values. The mean absolute error, or MAE, is the average of the absolute deviations between the expected and actual values. MAE is simpler to understand and considers all faults equally, in contrast to RMSE. Among the approaches, the MAE for VSHD is the lowest at 0.12. This implies even more that the imputed values are closer to the real ones and are therefore more accurate. Variable selection and the hot deck technique are combined in VSHD. In addition to lowering noise and enhancing the accuracy of the imputed values, variable selection aids in determining which variables are most pertinent for imputation. In contrast, the hot deck method fills in missing values with like, seen data points, which frequently yields more realistic and contextually relevant imputations. In order to adapt the imputation process according to the both variable selection and hot deck approaches. This results in imputations that are more accurate and contextually relevant. In conclusion, VSHD performs better than the other imputation techniques in terms of both RMSE and MAE, suggesting that it offers imputations that are more trustworthy and accurate. For managing missing data in the dataset under evaluation, this makes it a better option. 4.3 Prediction results by SHEM The accuracy of all the algorithms has demonstrated a marked improvement. Notably, after the application of VSHD, the Random forest distinguished itself by boosting its accuracy from 0.91 to 0.97, as illustrated in Fig. 6 a-6b. Tables 7a and 7b presented the performance metrics for four base learners, namely Support Vector Machine (SVM), Random Forest (RF), Light Gradient Boosting Machine (LGBM), Decision Tree (DT), and SHEM. The Decision Tree (DT) model achieves an (R-squared) value of 0.473, indicating it explains around 47.3 % of the variance in the dependent variable. Meanwhile, it records a Root Mean Squared Error (RMSE) of 0.679, hinting at an average error magnitude in its predictions of this value. Random Forest (RF) offers an R-squared of 0.752, suggesting it captures about 75.2 % of the variance in the dataset. However, it presents a slightly higher RMSE of 0.834, indicative of the prediction's average error magnitude. The Light Gradient Boosting Machine (LGBM) has an R-squared of 0.698, accounting for approximately 69.8 % of the variance. The RMSE for LGBM stands at 0.673, which is relatively lower, denoting better prediction accuracy when compared to RF. The Support Vector Machine. (SVM) demonstrates an R-squared value of 0.436, indicating its ability to account for approximately 43.6 % of the variance. However, its higher RMSE of 0.872 indicates a more significant average error in its predictions. Among all, SHEM outshines with an impressive R-squared value of 0.952. This high value suggests that SHEM captures a substantial 95.2 % of the variance in the dataset. Additionally, its minimal RMSE of 0.321 indicates a high prediction accuracy, dwarfing the errors observed in the other models. Overall, judging by the metrics provided, SHEM seems to be the most proficient model among the five, delivering exceptional variance explanation and accuracy in its predictions. The figures (6a−6d) present graph of a comparison of the actual and forecasted rainfall data using a Random Forest model and the VSHD (Variable Selection and Hot Deck) imputation method. Also it shows comparison of the actual and forecasted rainfall data before and after applying feature selection using Random forest machine learning model. The measured rainfall (blue) and forecasted rainfall (red) closely match each other, demonstrating the efficacy of the imputation and prediction techniques in identifying the underlying trends in the data. The graph 6a\u2013d shows a strong alignment between the observed and predicted values during the specified time frame, indicating that the VSHD imputation approach in conjunction with a Random Forest model as well as feature selection are effective in predicting rainfall data. Table 7a presents a comparative examination of diverse machine learning models. Base learners) and their performance metrics, including Accuracy, Kappa coefficient, and F1-Score. The DT model achieves an accuracy of 87 %, with a Kappa coefficient of 0.74, suggesting moderate agreement. The F1-Score, striking a balance between precision and recall, is recorded at 0.78. The RF model slightly outperforms the DT with an accuracy of 88 %. It also has a higher Kappa coefficient at 0.76 and an impressive F1-Score of 0.89, indicating robust precision and recall capabilities. LGBM's accuracy is lower at 78 %. Interestingly, it has a high Kappa coefficient of 0.87, suggesting substantial agreement between predicted and actual values. Its F1-Score is at 0.74. SVM showcases an accuracy of 86 %, coupled with a Kappa coefficient of 0.79. Its F1-Score is commendable at 0.87. The meta-classifier, XGBoost, plays a crucial role in the Stacked Heterogeneous Ensemble Model (SHEM) by combining the predictions from the base learners to make the final prediction. XGBoost, short for Extreme Gradient Boosting, is an optimized implementation of gradient boosting algorithms designed to provide high performance and scalability. It is widely used in machine learning competitions and real-world applications due to its effectiveness and efficiency. Table 7b presents the error metrics of various base learners employed in a predictive modeling task. The metrics evaluated are R-squared and Root Mean Squared Error (RMSE), commonly used to assess the performance of regression models. The decision tree (DT) model achieved an R-squared value of 0.473 and an RMSE of 0.679, indicating moderate predictive accuracy. Random Forest (RF) outperformed DT significantly with an R-squared of 0.752 and RMSE of 0.834, demonstrating better predictive power. LightGBM (LGBM) also performed well with an R-squared of 0.698 and an RMSE of 0.673, showcasing its effectiveness in handling large datasets efficiently. Support Vector Machine (SVM) yielded an R-squared of 0.436 and an RMSE of 0.872, suggesting relatively weaker predictive performance than other models. Finally, the Stacked Heterogeneous Ensemble Model (SHEM) displayed exceptional predictive accuracy with an impressive R-squared of 0.952 and the lowest RMSE of 0.321, indicating its superiority in capturing complex relationships within the data. Overall, the table underscores the varying performance levels of different base learners, with SHEM emerging as the top performer in R-squared and RMSE. 4.4 XGBoost as a Meta-Classifier In this role, XGBoost combines the predictions of multiple base learners to make the final prediction. Each base learner's prediction serves as a feature for the meta-classifier. The meta-classifier (often another XGBoost model) takes these base learner predictions as input features and learns to make the final prediction based on them. Mathematically, the prediction of the meta-classifier y ` meta ( x ) can be represented as: (3) y ` meta ( x ) = σ ∑ i = 1 N w i · y ` i ( x ) where w i the weights are assigned to the predictions of individual base learners, y ` i ( x ) are the predictions of the base learners, and σ ( · ) is the activation function (sigmoid for binary classification in this case). XGBoost is particularly well-suited for regression and classification tasks, making it a suitable choice as the meta-classifier in ensemble models like SHEM. XGBoost belongs to the family of gradient boosting algorithms, which sequentially combines weak learners (usually decision trees) to build a strong predictive model. It optimizes a predefined loss function by iteratively adding new models to minimize the residual errors. If the Root Mean Square Error (RMSE) is lower during the testing phase than in the training phase, it represents an atypical scenario. Fig. 7 a and b typically illustrates that a model is anticipated to exhibit superior, or at the very least, comparable performance on training data about unseen data (testing data). In an LSTM, the RMSE value during testing decreases as the number of neurons increases. If the RMSE value during testing decreases (improving model performance on unseen data) but the RMSE during training increases (worsening model performance on training data), the model might be underfitting. When the RMSE (Root Mean Square Error) value increases during the testing phase while likely being lower during the training phase, it indicates that the model memorizes the training data rather than deriving generalizations from it. This behaviour is indicative of overfitting. So, the model is overfitting due to increasing the number of layers in LSTM. From Fig. 7c and 7d, it's evident that when the number of epochs increases, leading to a rise in RMSE during training and a decline during testing, this pattern suggests a progression towards an optimal or \u201cbest fit\u201d model rather than drifting into overfitting or underfitting. SHEM involves stacking various base learners. Each learner might capture different patterns or nuances in the training data. The integrated knowledge from these diverse models can help the meta-model of SHEM generalize better across both training and testing datasets, reducing discrepancies in performance. The meta-model in SHEM learns how to optimally combine the predictions of the base learners. If certain base learners are overfitting to the training data's noise or anomalies, the meta-model can reduce their influence in the final prediction, relying more on models that generalize well. The meta-model in SHEM learns how to optimally combine the predictions of the base learners. If certain base learners are overfitting to the training data's noise or anomalies, the meta-model can reduce their influence in the final prediction, relying more on models that generalize well. If individual models within SHEM exhibit high variance between training and testing RMSE, consider retraining them with modified hyperparameters or even different algorithms. Continuously feed the outcomes of the SHEM's predictions back into the training process, allowing the model to iteratively refine its understanding and adapt to the data's nuances better. In essence, SHEM's integrated and hierarchical structure can help rectify discrepancies in RMSE values between training and testing phases. By leveraging the collective strength of diverse base learners and optimizing their combined predictions, SHEM ensures more consistent and reliable performance across different data sets. 5 Conclusion and future enhancement The proposed SHEM model showcases an impressive performance with an accuracy rate of 97 %.SHEM stands out with the highest performance across all metrics. It boasts an accuracy of 96 %, a near-perfect Kappa coefficient of 0.97, indicating almost complete agreement, and a high F1-Score of 0.94. In summary, while individual base learners demonstrate commendable performance, the ensemble model SHEM outshines all, consolidating the strengths of individual models to achieve superior results. Furthermore, one of the standout features of the SHEM model is its robustness in eliminating overfitting, which is often a prevalent challenge in machine learning. This ensures that the model's predictions remain reliable and consistent across varied datasets. The number of base models, their configurations, and the meta-learner are examples of hyperparameters that impact the SHEM model's performance. But integrating numerous models raises the possibility of overfitting, particularly if they aren't adequately verified using a variety of datasets. Exploration of additional base learners Further investigation into integrating novel machine learning algorithms could enhance SHEM's diversity and predictive power. Dynamic ensemble adaptation Research into methods for dynamically adapting the ensemble composition based on evolving data patterns and environmental conditions could improve the adaptability and responsiveness of SHEM. Funding information No funding is provided for the preparation of manuscript. CRediT authorship contribution statement P. Umamaheswari: Project administration, Investigation, Resources, Writing \u2013 original draft, Data curation, Visualization, Supervision, Conceptualization. V. Ramaswamy: Methodology, Formal analysis, Investigation, Data curation, Software, Validation, Formal analysis, Visualization, Writing \u2013 review & editing. Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. References Adnan et al., 2021 R.M. Adnan A. Petroselli S. Heddam C.A.G. Santos O. Kisi Short term rainfall-runoff modelling using several machine learning methods and a conceptual event-based model Stochastic Environmental Research and Risk Assessment 35 3 2021 597 616 Adnan, R. M., Petroselli, A., Heddam, S., Santos, C. A. G., & Kisi, O. (2021). Short term rainfall-runoff modelling using several machine learning methods and a conceptual event-based model. Stochastic Environmental Research and Risk Assessment, 35(3), 597-616. Barrera-Animas et al., 2022 A.Y. Barrera-Animas L.O. Oyedele M. Bilal T.D. Akinosho J.M.D. Delgado L.A. Akanbi Rainfall prediction: A comparative analysis of modern machine learning algorithms for time-series forecasting Machine Learning with Applications 7 2022 100204 Barrera-Animas, A. Y., Oyedele, L. O., Bilal, M., Akinosho, T. D., Delgado, J. M. D., & Akanbi, L. A. (2022). Rainfall prediction: A comparative analysis of modern machine learning algorithms for time-series forecasting. Machine Learning with Applications, 7, 100204. da Silva et al., 2022 R.G. da Silva S.R. Moreno M.H.D.M. Ribeiro J.H.K. Larcher V.C. Mariani L. dos Santos Coelho Multi-step short-term wind speed forecasting based on multi-stage decomposition coupled with stacking-ensemble learning approach International Journal of Electrical Power & Energy Systems 143 2022 108504 da Silva, R. G., Moreno, S. R., Ribeiro, M. H. D. M., Larcher, J. H. K., Mariani, V. C., & dos Santos Coelho, L. (2022). Multi-step short-term wind speed forecasting based on multi-stage decomposition coupled with stacking-ensemble learning approach. International Journal of Electrical Power & Energy Systems, 143, 108504. Danandeh Mehr et al., 2023 A. Danandeh Mehr A. Rikhtehgar Ghiasi Z.M. Yaseen A.U. Sorman L. Abualigah A novel intelligent deep learning predictive model for meteorological drought forecasting Journal of Ambient Intelligence and Humanized Computing 14 8 2023 10441 10455 Danandeh Mehr, A., Rikhtehgar Ghiasi, A., Yaseen, Z. M., Sorman, A. U., & Abualigah, L. (2023). A novel intelligent deep learning predictive model for meteorological drought forecasting. Journal of Ambient Intelligence and Humanized Computing, 14(8), 10441-10455. Espeholt et al., 2022 L. Espeholt S. Agrawal C. Sønderby M. Kumar J. Heek C. Bromberg N. Kalchbrenner Deep learning for twelve hour precipitation forecasts Nature Communications 13 1 2022 1 10 Espeholt, L., Agrawal, S., Sønderby, C., Kumar, M., Heek, J., Bromberg, C., ... & Kalchbrenner, N. (2022). Deep learning for twelve hour precipitation forecasts. Nature communications, 13(1), 1-10. Evans et al., 2020 F.H. Evans M.M. Guthrie I. Foster Accuracy of six years of operational statistical seasonal forecasts of rainfall in Western Australia (2013 to 2018) Atmospheric Research 233 2020 104697 Evans, F. H., Guthrie, M. M., & Foster, I. (2020). Accuracy of six years of operational statistical seasonal forecasts of rainfall in Western Australia (2013 to 2018). Atmospheric Research, 233, 104697. Fahad et al., 2023 S. Fahad F. Su S.U. Khan M.R. Naeem K. Wei Implementing a novel deep learning technique for rainfall forecasting via climatic variables: An approach via hierarchical clustering analysis Science of The Total Environment 854 2023 158760 Fahad, S., Su, F., Khan, S. U., Naeem, M. R., & Wei, K. (2023). Implementing a novel deep learning technique for rainfall forecasting via climatic variables: An approach via hierarchical clustering analysis. Science of The Total Environment, 854, 158760. Flores et al., 2023 M. Flores Á. Llambo D. Loza S. Naya J. Tarrío-Saavedra Predicting rainfall and irrigation requirements of corn in Ecuador Heliyon 9 8 2023 Flores, M., Llambo, Á., Loza, D., Naya, S., & Tarrío-Saavedra, J. (2023). Predicting rainfall and irrigation requirements of corn in Ecuador. Heliyon, 9(8). Chagahi et al., 2024 M.H. Chagahi S.M. Dashtaki B. Moshiri M.J. Piran Cardiovascular disease detection using a novel stack-based ensemble classifier with aggregation layer, DOWA operator, and feature transformation Computers in Biology and Medicine 173 2024 108345 Chagahi, M. H., Dashtaki, S. M., Moshiri, B., & Piran, M. J. (2024). Cardiovascular disease detection using a novel stack-based ensemble classifier with aggregation layer, DOWA operator, and feature transformation. Computers in Biology and Medicine, 173, 108345. Jose et al., 2022 D.M. Jose A.M. Vincent G.S. Dwarakish Improving multiple model ensemble predictions of daily precipitation and temperature through machine learning techniques Scientific Reports 12 1 2022 4678 Jose, D. M., Vincent, A. M., & Dwarakish, G. S. (2022). Improving multiple model ensemble predictions of daily precipitation and temperature through machine learning techniques. Scientific Reports, 12(1), 4678. Ko et al., 2022 J. Ko K. Lee H. Hwang S.G. Oh S.W. Son K. Shin Effective training strategies for deep-learning-based precipitation nowcasting and estimation Computers & Geosciences 161 2022 105072 Ko, J., Lee, K., Hwang, H., Oh, S. G., Son, S. W., & Shin, K. (2022). Effective training strategies for deep-learning-based precipitation nowcasting and estimation. Computers & Geosciences, 161, 105072. Li, 2021 J. Li Exploring the potential of utilizing unsupervised machine learning for urban drainage sensor placement under future rainfall uncertainty Journal of Environmental Management 296 2021 113191 Li, J. (2021). Exploring the potential of utilizing unsupervised machine learning for urban drainage sensor placement under future rainfall uncertainty. Journal of Environmental Management, 296, 113191. Luo et al., 2024 Y. Luo W. Chen L. Zhan J. Qiu T. Jia Multi-feature concatenation and multi-classifier stacking: An interpretable and generalizable machine learning method for MDD discrimination with rsfMRI NeuroImage 285 2024 120497 Luo, Y., Chen, W., Zhan, L., Qiu, J., & Jia, T. (2024). Multi-feature concatenation and multi-classifier stacking: an interpretable and generalizable machine learning method for MDD discrimination with rsfMRI. NeuroImage, 285, 120497. Mohapatra et al., 2023 S. Mohapatra S. Maneesha S. Mohanty P.K. Patra S.K. Bhoi K.S. Sahoo A.H. Gandomi A stacking classifiers model for detecting heart irregularities and predicting Cardiovascular Disease Healthcare Analytics 3 2023 100133 Mohapatra, S., Maneesha, S., Mohanty, S., Patra, P. K., Bhoi, S. K., Sahoo, K. S., & Gandomi, A. H. (2023). A stacking classifiers model for detecting heart irregularities and predicting Cardiovascular Disease. Healthcare Analytics, 3, 100133. Narang et al., 2024 U. Narang K. Juneja P. Upadhyaya P. Salunke T. Chakraborty S.K. Behera A.D. Suresh Artificial intelligence predicts normal summer monsoon rainfall for India in 2023 Scientific Reports 14 1 2024 1495 Narang, U., Juneja, K., Upadhyaya, P., Salunke, P., Chakraborty, T., Behera, S. K., ... & Suresh, A. D. (2024). Artificial intelligence predicts normal summer monsoon rainfall for India in 2023. Scientific Reports, 14(1), 1495. Ouma et al., 2021 Y.O. Ouma R. Cheruyot A.N. Wachera Rainfall and runoff time-series trend analysis using LSTM recurrent neural network and wavelet neural network with satellite-based meteorological data: Case study of Nzoia hydrologic basin Complex & Intelligent Systems 2021 1 24 Ouma, Y. O., Cheruyot, R., & Wachera, A. N. (2021). Rainfall and runoff time-series trend analysis using LSTM recurrent neural network and wavelet neural network with satellite-based meteorological data: case study of Nzoia hydrologic basin. Complex & Intelligent Systems, 1-24. Panda et al., 2024 J. Panda N. Nagar A. Mukherjee S. Bhattacharyya S. Singh Rainfall variability over multiple cities of India: Analysis and forecasting using deep learning models Earth Science Informatics 2024 1 20 Panda, J., Nagar, N., Mukherjee, A., Bhattacharyya, S., & Singh, S. (2024). Rainfall variability over multiple cities of India: analysis and forecasting using deep learning models. Earth Science Informatics, 1-20. Peñalvo et al., 2022 F.J.G. Peñalvo T. Maan S.K. Singh S. Kumar V. Arya K.T. Chui G.P. Singh Sustainable stock market prediction framework using machine learning models International Journal of Software Science and Computational Intelligence (IJSSCI) 14 1 2022 1 15 Peñalvo, F. J. G., Maan, T., Singh, S. K., Kumar, S., Arya, V., Chui, K. T., & Singh, G. P. (2022). Sustainable stock market prediction framework using machine learning models. International Journal of Software Science and Computational Intelligence (IJSSCI), 14(1), 1-15. Rahman et al., 2022 A.U. Rahman S. Abbas M. Gollapalli R. Ahmed S. Aftab M. Ahmad A. Mosavi Rainfall prediction system using machine learning fusion for smart cities Sensors 22 9 2022 3504 Rahman, A. U., Abbas, S., Gollapalli, M., Ahmed, R., Aftab, S., Ahmad, M., ... & Mosavi, A. (2022). Rainfall prediction system using machine learning fusion for smart cities. Sensors, 22(9), 3504. Rampal et al., 2022 N. Rampal P.B. Gibson A. Sood S. Stuart N.C. Fauchereau C. Brandolino T. Meyers High-resolution downscaling with interpretable deep learning: Rainfall extremes over New Zealand Weather and Climate Extremes 38 2022 100525 Rampal, N., Gibson, P. B., Sood, A., Stuart, S., Fauchereau, N. C., Brandolino, C., ... & Meyers, T. (2022). High-resolution downscaling with interpretable deep learning: Rainfall extremes over New Zealand. Weather and Climate Extremes, 38, 100525. Ren et al., 2021 X. Ren X. Li K. Ren J. Song Z. Xu K. Deng X. Wang Deep learning-based weather prediction: A survey Big Data Research 23 2021 100178 Ren, X., Li, X., Ren, K., Song, J., Xu, Z., Deng, K., & Wang, X. (2021). Deep learning-based weather prediction: a survey. Big Data Research, 23, 100178. Ribeiro et al., 2022 M.H.D.M. Ribeiro R.G. da Silva S.R. Moreno V.C. Mariani L. dos Santos Coelho Efficient bootstrap stacking ensemble learning model applied to wind power generation forecasting International Journal of Electrical Power & Energy Systems 136 2022 107712 Ribeiro, M. H. D. M., da Silva, R. G., Moreno, S. R., Mariani, V. C., & dos Santos Coelho, L. (2022). Efficient bootstrap stacking ensemble learning model applied to wind power generation forecasting. International Journal of Electrical Power & Energy Systems, 136, 107712. Ridwan et al., 2021 W.M. Ridwan M. Sapitang A. Aziz K.F. Kushiar A.N. Ahmed A. El-Shafie Rainfall forecasting model using machine learning methods: Case study Terengganu, Malaysia Ain Shams Engineering Journal 12 2 2021 1651 1663 Ridwan, W. M., Sapitang, M., Aziz, A., Kushiar, K. F., Ahmed, A. N., & El-Shafie, A. (2021). Rainfall forecasting model using machine learning methods: Case study Terengganu, Malaysia. Ain Shams Engineering Journal, 12(2), 1651-1663. Wei and You, 2022 M. Wei X.Y. You Monthly rainfall forecasting by a hybrid neural network of discrete wavelet transformation and deep learning Water Resources Management 36 11 2022 4003 4018 Wei, M., & You, X. Y. (2022). Monthly rainfall forecasting by a hybrid neural network of discrete wavelet transformation and deep learning. Water Resources Management, 36(11), 4003-4018. Wu et al., 2020 Z. Wu Y. Zhou H. Wang Z. Jiang Depth prediction of urban flood under different rainfall return periods based on deep learning and data warehouse Science of The Total Environment 716 2020 137077 Wu, Z., Zhou, Y., Wang, H., & Jiang, Z. (2020). Depth prediction of urban flood under different rainfall return periods based on deep learning and data warehouse. Science of The Total Environment, 716, 137077. Yin et al., 2023 H. Yin F. Zheng H.F. Duan D. Savic Z. Kapelan Estimating rainfall intensity using an image-based deep learning model Engineering 21 2023 162 174 Yin, H., Zheng, F., Duan, H. F., Savic, D., & Kapelan, Z. (2023). Estimating rainfall intensity using an image-based deep learning model. Engineering, 21, 162-174. Zhang et al., 2020 W. Zhang D. Liu S. Zheng S. Liu H.A. Loáiciga W. Li Regional precipitation model based on geographically and temporally weighted regression kriging Remote Sensing 12 16 2020 2547 Zhang, W., Liu, D., Zheng, S., Liu, S., Loáiciga, H. A., & Li, W. (2020). Regional precipitation model based on geographically and temporally weighted regression kriging. Remote Sensing, 12(16), 2547.",
    "scopus-id": "85200440099",
    "coredata": {
        "eid": "1-s2.0-S0957417424016981",
        "dc:description": "Recent climate conditions in India have led to numerous disasters, including floods, overflowing rivers, broken dams, and reduced vegetation. Machine learning aims to train models to make predictions based on historical data. Although many studies have focused on predictions and model building, this research introduces a novel integrated framework using a Stacked Heterogeneous Ensemble Model (SHEM). It aims to enhance prediction accuracy by employing another novel approach for imputing missing values, the Variable Specific Hot Deck (VSHD) imputation method. The outcomes were contrasted with established machine learning techniques, including random forest, decision tree, k-nearest neighbor, and support vector machine. After completing the imputation, we proceeded to implement the SHEM model. Real-time climate data for the Cuddalore location in Tamil Nadu state, South India was collected from the NASA Power Access viewer portal to verify the accuracy level of the proposed model. Performance analysis indicates that the proposed imputation outperforms all four alternative models with a 30% average improvement in accuracy. Moreover, the developed SHEM model has a reduced RMSE value of 0.321 and an R-squared value of 0.952 and shows a 9% improvement in accuracy compared to the base model\u2019s performance. Furthermore, these prediction results are compared to an LSTM (Long Short Term Memory), a deep learning model to calculate accuracy and loss, showing that the proposed achieves high accuracy and significant loss during validation. The obtained results can serve as a guideline for atmospheric scientists and various weather forecasting applications, helping them to choose the most appropriate machine learning method for their prediction task.",
        "openArchiveArticle": "false",
        "prism:coverDate": "2024-12-05",
        "openaccessUserLicense": null,
        "prism:aggregationType": "Journal",
        "prism:url": "https://api.elsevier.com/content/article/pii/S0957417424016981",
        "dc:creator": [
            {
                "@_fa": "true",
                "$": "Umamaheswari, P."
            },
            {
                "@_fa": "true",
                "$": "Ramaswamy, V."
            }
        ],
        "link": [
            {
                "@_fa": "true",
                "@rel": "self",
                "@href": "https://api.elsevier.com/content/article/pii/S0957417424016981"
            },
            {
                "@_fa": "true",
                "@rel": "scidir",
                "@href": "https://www.sciencedirect.com/science/article/pii/S0957417424016981"
            }
        ],
        "dc:format": "application/json",
        "openaccessType": null,
        "pii": "S0957-4174(24)01698-1",
        "prism:volume": "256",
        "articleNumber": "124831",
        "prism:publisher": "Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
        "dc:title": "An integrated framework for rainfall prediction and analysis using a Stacked Heterogeneous Ensemble Model (SHEM)",
        "prism:copyright": "© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",
        "openaccess": "0",
        "prism:issn": "09574174",
        "dcterms:subject": [
            {
                "@_fa": "true",
                "$": "Rainfall prediction"
            },
            {
                "@_fa": "true",
                "$": "Weather forecasting"
            },
            {
                "@_fa": "true",
                "$": "Imputation"
            },
            {
                "@_fa": "true",
                "$": "Machine learning"
            },
            {
                "@_fa": "true",
                "$": "Deep learning"
            },
            {
                "@_fa": "true",
                "$": "Regression"
            },
            {
                "@_fa": "true",
                "$": "Stacking ensemble model"
            }
        ],
        "openaccessArticle": "false",
        "prism:publicationName": "Expert Systems with Applications",
        "openaccessSponsorType": null,
        "prism:pageRange": "124831",
        "pubType": "fla",
        "prism:coverDisplayDate": "5 December 2024",
        "prism:doi": "10.1016/j.eswa.2024.124831",
        "prism:startingPage": "124831",
        "dc:identifier": "doi:10.1016/j.eswa.2024.124831",
        "openaccessSponsorName": null
    },
    "objects": {"object": [
        {
            "@category": "standard",
            "@height": "345",
            "@width": "552",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr2.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "53840",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "200",
            "@width": "415",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-ga1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "28016",
            "@ref": "ga1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "219",
            "@width": "621",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr3.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "22375",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "477",
            "@width": "528",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr4.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "58000",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "264",
            "@width": "546",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr5.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "18551",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "279",
            "@width": "638",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr1.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "29284",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "453",
            "@width": "638",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr6.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "71978",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "standard",
            "@height": "535",
            "@width": "651",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr7.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-DOWNSAMPLED",
            "@size": "56664",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@height": "137",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr2.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12331",
            "@ref": "gr2",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "106",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-ga1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "12708",
            "@ref": "ga1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "77",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr3.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "3958",
            "@ref": "gr3",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "181",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr4.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "9859",
            "@ref": "gr4",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "106",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr5.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "4169",
            "@ref": "gr5",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "96",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr1.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "5346",
            "@ref": "gr1",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "156",
            "@width": "219",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr6.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "11350",
            "@ref": "gr6",
            "@mimetype": "image/gif"
        },
        {
            "@category": "thumbnail",
            "@height": "163",
            "@width": "199",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr7.sml?httpAccept=%2A%2F%2A",
            "@multimediatype": "GIF image file",
            "@type": "IMAGE-THUMBNAIL",
            "@size": "7361",
            "@ref": "gr7",
            "@mimetype": "image/gif"
        },
        {
            "@category": "high",
            "@height": "1528",
            "@width": "2443",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr2_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "277965",
            "@ref": "gr2",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "886",
            "@width": "1837",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-ga1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "180928",
            "@ref": "ga1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "970",
            "@width": "2751",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr3_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "130859",
            "@ref": "gr3",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2110",
            "@width": "2337",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr4_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "334614",
            "@ref": "gr4",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1168",
            "@width": "2418",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr5_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "91685",
            "@ref": "gr5",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "1236",
            "@width": "2826",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr1_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "158888",
            "@ref": "gr1",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2008",
            "@width": "2827",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr6_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "416255",
            "@ref": "gr6",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "high",
            "@height": "2367",
            "@width": "2881",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-gr7_lrg.jpg?httpAccept=%2A%2F%2A",
            "@multimediatype": "JPEG image file",
            "@type": "IMAGE-HIGH-RES",
            "@size": "314957",
            "@ref": "gr7",
            "@mimetype": "image/jpeg"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si1.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "12972",
            "@ref": "si1",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si10.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19799",
            "@ref": "si10",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si11.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3553",
            "@ref": "si11",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si12.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "33267",
            "@ref": "si12",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si13.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "11547",
            "@ref": "si13",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si14.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "5361",
            "@ref": "si14",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si15.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6992",
            "@ref": "si15",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si16.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "10085",
            "@ref": "si16",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si17.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "19758",
            "@ref": "si17",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si18.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3620",
            "@ref": "si18",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si19.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6798",
            "@ref": "si19",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si2.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "14605",
            "@ref": "si2",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si20.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "3201",
            "@ref": "si20",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si3.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9757",
            "@ref": "si3",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si4.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "6450",
            "@ref": "si4",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si5.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8831",
            "@ref": "si5",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si6.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "8414",
            "@ref": "si6",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si7.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "9313",
            "@ref": "si7",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si8.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "7605",
            "@ref": "si8",
            "@mimetype": "image/svg+xml"
        },
        {
            "@category": "thumbnail",
            "@_fa": "true",
            "$": "https://api.elsevier.com/content/object/eid/1-s2.0-S0957417424016981-si9.svg?httpAccept=%2A%2F%2A",
            "@multimediatype": "Scalable Vector Graphics file",
            "@type": "ALTIMG",
            "@size": "20296",
            "@ref": "si9",
            "@mimetype": "image/svg+xml"
        }
    ]},
    "link": {
        "@rel": "abstract",
        "@href": "https://api.elsevier.com/content/abstract/scopus_id/85200440099"
    }
}}